
@back

Je veux que tu amÃ©liores le backend Django du projet **Webtoon Book** avec une nouvelle feature :  
ğŸ§© **Un Scrapper automatique de webtoons**, inspirÃ© du repo https://github.com/unclecode/crawl4ai.

Cette feature sera accessible depuis le menu latÃ©ral du front (bouton â€œScrapperâ€ ou â€œExtraction Webtoonâ€).  
Elle permettra Ã  lâ€™utilisateur dâ€™entrer le lien dâ€™un webtoon (ex : https://manga-scantrad.io/manga/le-retour-du-ranker/),  
et le backend devra :
- Scrapper tous les chapitres du webtoon (titres, liens, images),
- TÃ©lÃ©charger toutes les images localement,
- CrÃ©er automatiquement les entrÃ©es `Webtoon` et `Chapter` dans la base Django,
- Retourner un rapport de succÃ¨s (nombre de chapitres et dâ€™images rÃ©cupÃ©rÃ©s),
- GÃ©rer les erreurs de scraping de maniÃ¨re robuste (retry, timeout, logs).

---

## ğŸ§± 1. Architecture Ã  mettre Ã  jour

Ajoute un **nouvel app Django** :
```

backend/
â”œâ”€ scraper/
â”‚    â”œâ”€ **init**.py
â”‚    â”œâ”€ crawler.py
â”‚    â”œâ”€ tasks.py
â”‚    â”œâ”€ serializers.py
â”‚    â”œâ”€ views.py
â”‚    â”œâ”€ urls.py
â”‚    â”œâ”€ models.py (optionnel si tu veux journaliser les scrapes)
â”‚    â””â”€ tests/test_scraper.py

````

---

## âš™ï¸ 2. Fonctionnement du Scrapper

Utilise la librairie `crawl4ai` pour explorer et extraire les contenus :

### Exemple dâ€™utilisation :
```python
from crawl4ai import WebCrawler

async def scrape_manga(url: str):
    async with WebCrawler() as crawler:
        result = await crawler.run(url)
        # RÃ©cupÃ©rer titres, images, liens
        for item in result['chapters']:
            print(item['title'], item['images'])
````

âš™ï¸ Si `crawl4ai` nâ€™est pas dispo, utilise `requests + BeautifulSoup4` comme fallback.

---

## ğŸ“¡ 3. Endpoint REST Ã  crÃ©er

| MÃ©thode | Endpoint                    | Description                                                |
| ------- | --------------------------- | ---------------------------------------------------------- |
| POST    | `/api/scraper/`             | Lance le scraping dâ€™un webtoon                             |
| GET     | `/api/scraper/status/{id}/` | RÃ©cupÃ¨re le statut du scraping (en cours, terminÃ©, Ã©chouÃ©) |
| GET     | `/api/scraper/history/`     | Liste les derniers scrapes rÃ©alisÃ©s                        |

### Exemple de payload :

```json
{
  "url": "https://manga-scantrad.io/manga/le-retour-du-ranker/"
}
```

### RÃ©ponse attendue :

```json
{
  "status": "success",
  "webtoon": "Le Retour du Ranker",
  "chapters_scraped": 128,
  "images_downloaded": 2400,
  "local_path": "/media/webtoons/le-retour-du-ranker/",
  "duration": "00:03:41"
}
```

---

## ğŸ§© 4. IntÃ©gration avec les modÃ¨les existants

Lors du scraping :

1. CrÃ©e ou rÃ©cupÃ¨re un `Webtoon` avec le mÃªme titre.
2. CrÃ©e chaque `Chapter` avec :

   * `title`
   * `chapter_number`
   * `release_date` (si dispo)
   * `local_folder` (chemin local vers les images)
3. Stocke les images sous :

```
/media/webtoons/<slug-du-webtoon>/<chapter-number>/
```

â¡ï¸ Les chemins seront accessibles via un champ `local_image_paths` dans les modÃ¨les.

---

## ğŸš€ 5. TÃ¢ches asynchrones (optionnel mais recommandÃ©)

Ajoute la possibilitÃ© de lancer le scraping en tÃ¢che de fond avec **Celery** :

* Le POST `/api/scraper/` crÃ©e une tÃ¢che Celery.
* Le front peut consulter `/api/scraper/status/{id}/` pour suivre la progression.

Celery + Redis :

```bash
pip install celery redis
celery -A webtoonbook worker --loglevel=INFO
```

---

## ğŸ§ª 6. Tests Ã  implÃ©menter

CrÃ©e des tests automatiques dans `tests/test_scraper.py` :

* `test_scraper_endpoint_exists`
* `test_scraper_invalid_url_returns_400`
* `test_scraper_creates_webtoon_and_chapters`
* `test_scraper_stores_images_locally`
* `test_scraper_status_returns_progress`

Tous les tests doivent passer avant validation finale :

```bash
pytest -v
# ou
python manage.py test scraper
```

---

## ğŸ“˜ 7. Documentation API

Ã‰tend la doc Swagger / ReDoc :

* `/api/docs/swagger/` â†’ inclure la section **Scraper**
* `/api/docs/redoc/` â†’ mÃªme chose

Ajoute une description claire :

> â€œPermet de scrapper automatiquement les chapitres et images dâ€™un webtoon Ã  partir dâ€™une URL (MangaScantrad, AsuraScans, etc.).â€

---

## ğŸ§­ 8. Lien avec le Frontend

Expose dans la doc un exemple dâ€™appel depuis le front :

```javascript
// frontend/src/api/scraper.ts
import axios from 'axios';

export const launchScraper = async (url: string) => {
  const response = await axios.post('/api/scraper/', { url });
  return response.data;
};
```

Dans la sidebar du front (menu â€œScrapperâ€ ou â€œFeature suivanteâ€):

* CrÃ©e un bouton **â€œLancer un scrapâ€**
* Champ `input` pour coller une URL
* Appel `launchScraper(url)`
* Affiche un **loader + rÃ©sultat** (â€œ128 chapitres ajoutÃ©s avec succÃ¨s !â€)

---

## ğŸ§¾ 9. Exemple de workflow complet

1. Lâ€™utilisateur va dans â€œScrapperâ€.
2. Il colle : `https://manga-scantrad.io/manga/le-retour-du-ranker/`.
3. Le front appelle `/api/scraper/` (POST).
4. Le backend tÃ©lÃ©charge tous les chapitres et images.
5. Les nouveaux webtoons et chapitres apparaissent dans la base.
6. Le front recharge `/api/webtoons/` â†’ ils sâ€™affichent dans la bibliothÃ¨que.

---

## âš™ï¸ 10. Configuration Docker & stockage local

Dans `docker-compose.yml`, ajoute un volume :

```yaml
volumes:
  webtoon_media:
    driver: local

services:
  web:
    volumes:
      - webtoon_media:/app/media
```

Et dans `settings.py` :

```python
MEDIA_URL = '/media/'
MEDIA_ROOT = BASE_DIR / 'media'
```

---

## âœ… 11. VÃ©rification finale avant livraison

1. Tous les tests passent (`pytest` OK)
2. Les endpoints `/api/webtoons/` et `/api/scraper/` fonctionnent ensemble
3. Swagger documente la nouvelle route
4. Le front peut lancer un scraping depuis un bouton et voir le rÃ©sultat
5. Les images apparaissent bien dans `/media/webtoons/...`
6. Aucune erreur critique dans les logs

---

## ğŸ“„ 12. Ã€ livrer

* Code complet `scraper/`
* Tests unitaires âœ…
* Migrations appliquÃ©es âœ…
* Doc Swagger/Redoc mise Ã  jour âœ…
* `README_BACKEND.md` mis Ã  jour avec instructions dâ€™utilisation du scrapper :

  * Comment lancer un scrap
  * OÃ¹ les fichiers sont stockÃ©s
  * Commandes Docker/Celery
  * Endpoints disponibles

---

RÃ©sultat attendu :
âœ… Tous les tests passent
âœ… API documentÃ©e
âœ… Scrapper opÃ©rationnel reliÃ© au front
âœ… Commande â€œAjouter un webtoonâ€ fonctionnelle
âœ… Images et chapitres visibles dans la bibliothÃ¨que

```

